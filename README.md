# Distributed Data Pipeline

A practical project designed to build a small-scale distributed data
processing pipeline using **Apache Spark**.

## Problem Statement
Processing large volumes of raw log data efficiently is a common challenge
in data-intensive systems. This project focuses on designing a batch-oriented
pipeline to ingest, process, and aggregate log data in a distributed manner.

## Planned Architecture
- Data Source: CSV / JSON log files
- Processing Engine: Apache Spark (Batch)
- Environment: Dockerized local cluster
- Output: Aggregated analytics data (Parquet)

## Key Learning Goals
- Understanding Spark execution model
- Partitioning and parallel data processing
- Basic fault tolerance and job retries
- Working with Linux-based environments

## Status
- Environment setup: In progress
- Initial Spark job: Pending
